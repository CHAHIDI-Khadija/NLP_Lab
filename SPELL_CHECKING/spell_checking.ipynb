{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spell checker 1 from scratch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name: str, printing: bool=False) -> str:\n",
    "    \"\"\"\n",
    "    Load the corpus data from a file and return it as a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "        file_name: Name of the file.\n",
    "        printing: If true, some information about the data will be printed.\n",
    "    \n",
    "    Return: \n",
    "        str: The loaded data as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding='utf-8') as f:\n",
    "            data = f.read().lower()\n",
    "\n",
    "        if printing:\n",
    "            print(\"Data type:\", type(data))\n",
    "            print(f\"Number of letters: {len(data):,d}\")\n",
    "            print(\"First 100 letters of the data\")\n",
    "            print(\"-\"*30)\n",
    "            display(data[0:100])\n",
    "            print(\"-\"*30)\n",
    "\n",
    "            print(\"Last 100 letters of the data\")\n",
    "            print(\"-\"*30)\n",
    "            display(data[-100:])\n",
    "            print(\"-\"*30)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Error: File '{file_name}' not found.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Input String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(input_str: str):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        input_str: a string to check the spell\n",
    "    Output: \n",
    "        cleaned_input: a list of lower case words of input_str, \n",
    "          the list dosen't contains any non-alphabetic characters\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = re.sub(r'\\W+|\\b\\d+\\b', ' ', input_str)\n",
    "    cleaned_input = pattern.lower().split()\n",
    "\n",
    "    return cleaned_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the word in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(word: str, vocabulary: set[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a word is in the vocabulary.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The word to check.\n",
    "        vocabulary (set[str]): The set of words representing the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word is in the dictionary, False otherwise.\n",
    "    \"\"\"\n",
    "    return word.lower() in vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(word_list: list[str]):\n",
    "    \"\"\"\n",
    "    Count the frequency of words in a list and return a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        word_list (list): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        The wordcount dictionary where key is the word and value is its frequency.\n",
    "    \"\"\"\n",
    "    return Counter(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probs calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    \"\"\"\n",
    "    Calculate the probability of each word based on its frequency.\n",
    "    \n",
    "    Parametres:\n",
    "        word_count_dict (dict): The wordcount dictionary where key is the word and value is its frequency.\n",
    "    \n",
    "    Returns:\n",
    "        probs (dict): A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    \"\"\"\n",
    "    \n",
    "    total_words = len(word_count_dict)\n",
    "    probs = {word: count/total_words for word, count in word_count_dict.items()}\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit_one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_v1(word: str):\n",
    "    \"\"\"\n",
    "    Generate a set of possible corrections for a misspelled word with one edit.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The misspelled word.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of possible corrections.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [left + right[1:] for left, right in splits if right]\n",
    "    inserts = [left + c + right for left, right in splits for c in alphabet]\n",
    "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabet]\n",
    "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
    "    \n",
    "    return set(deletes + inserts + replaces + transposes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corrections_v1(word: str, n_edits: int):\n",
    "    \"\"\"\n",
    "    Generate a set of possible corrections for a misspelled word using the edit distance algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The misspelled word.\n",
    "        n_edits (int): Number of edits.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of possible corrections.\n",
    "    \"\"\"\n",
    "    \n",
    "    edits = set()\n",
    "\n",
    "    if n_edits == 1:\n",
    "        edits = edit_one_v1(word)\n",
    "    else:\n",
    "        edit_set = {word}\n",
    "        for i in range(n_edits):\n",
    "            edit_iter = set()\n",
    "            for w in edit_set:\n",
    "                edit_iter.update(edit_one_v1(w))\n",
    "            edit_set = edit_iter\n",
    "        edits = edit_set\n",
    "\n",
    "    return edits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_v1(text: str, vocab: list[str], probs: dict, n_edits: int=1, n: int=2):\n",
    "    \"\"\"\n",
    "    Check the spelling of a piece of text.\n",
    "\n",
    "    Parameters:\n",
    "        text: The text to check;\n",
    "        vocab: list of words, vocabulary;\n",
    "        probs: Dictionary of word probabilities;\n",
    "        n_edits: maximum number of edits;\n",
    "        n: number of possible word corrections you want returned in the dictionar.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of misspelled words and their possible corrections.\n",
    "    \"\"\"\n",
    "\n",
    "    words = clean_input(text)\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    corrected_text = text\n",
    "\n",
    "    for word in words:\n",
    "        if not is_word(word, vocab):\n",
    "            corrections = []\n",
    "            a = 1\n",
    "            while not corrections:\n",
    "                if a > n_edits: break\n",
    "                corrections = generate_corrections_v1(word, a)\n",
    "                a += 1\n",
    "            tmp_suggestions = {c: probs[c] for c in corrections if is_word(c, vocab)}\n",
    "            suggestions.append((word, tmp_suggestions))\n",
    "\n",
    "    suggestions_sorted = [[word, dict(sorted(sugg.items(), key=lambda x: x[1], reverse=True))] for word, sugg in suggestions]\n",
    "\n",
    "    corrected_sentence = text.split()\n",
    "    for i, (word, sugg) in enumerate(suggestions_sorted):\n",
    "        top_n_sugg = dict(sorted(sugg.items(), key=lambda x: x[1], reverse=True)[:n])\n",
    "        n_best.append((word, dict(top_n_sugg)))\n",
    "\n",
    "        if top_n_sugg:\n",
    "            best_correction = next(iter(top_n_sugg))\n",
    "            corrected_sentence[i] = best_correction\n",
    "            corrected_text = corrected_text.replace(word, best_correction)\n",
    "\n",
    "    return n_best, corrected_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36894 unique words in the vocabulary.\n",
      "The count for the word 'say' is 238\n"
     ]
    }
   ],
   "source": [
    "data = load_data('en_US_twitter.txt')\n",
    "word_list = re.findall('\\w+', data)\n",
    "word_vocab = set(word_list)\n",
    "word_count_dict = count(word_list)\n",
    "probs = get_probs(word_count_dict)\n",
    "\n",
    "print(f\"There are {len(word_vocab)} unique words in the vocabulary.\")\n",
    "print(f\"The count for the word 'say' is {word_count_dict.get('name',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"In Krav Maga thre are no rulse, no restrctions.\\nI actaully lked Derek Morris as a Ranger.\"\n",
    "text2 = \"Thanks for the quck birhday lessn\"\n",
    "\n",
    "n_best, corrected_sentence = spell_check_v1(text1, word_vocab, probs, n_edits=1, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      " In Krav Maga thre are no rulse, no restrctions.\n",
      "I actaully lked Derek Morris as a Ranger.\n",
      "\n",
      "Corrected sentence:\n",
      " In Krav Maga the are no rules, no restrictions.\n",
      "I actaully liked Derek Morris as a Ranger.\n",
      "\n",
      "Suggestions for misspelled words:\n",
      "\n",
      "  thre :\n",
      "\tthe: 0.5175909361955874\n",
      "\tthere: 0.039112050739957716\n",
      "\n",
      "  rulse :\n",
      "\trules: 0.0010028730958963517\n",
      "\trule: 0.0008131403480240689\n",
      "\n",
      "  restrctions :\n",
      "\trestrictions: 5.420935653493793e-05\n",
      "\n",
      "  lked :\n",
      "\tliked: 0.001599176017780669\n",
      "\tled: 0.0003794654957445655\n"
     ]
    }
   ],
   "source": [
    "print(\"Original sentence:\\n\", text1, end='\\n'*2)\n",
    "print(\"Corrected sentence:\\n\", corrected_sentence, end='\\n'*2)\n",
    "print(\"Suggestions for misspelled words:\")\n",
    "\n",
    "for m in n_best:\n",
    "    print(f\"\\n  {m[0]} :\")\n",
    "    for w, p in m[1].items():\n",
    "        print(f\"\\t{w}: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spell checker 2 with minimum edit distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by linebreak \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Parameters\n",
    "        data (str): The input data as a string.\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: A list of sentences\n",
    "    \"\"\"\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizes a sentence into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a sentence into tokens.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): The input sentence.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of tokens.\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    tokens = sentence.split()\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of tokenized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data: str) -> tuple[list[str], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenize the data into sentences and words.\n",
    "\n",
    "    Parameters:\n",
    "        data (str): The input data as a string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[str], list[list[str]]]: A tuple containing the list of sentences and list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = [tokenize_sentence(s) for s in sentences]\n",
    "    \n",
    "    return sentences, tokenized_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_data: list[list[str]]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Extract the vocabulary from a list of tokenized data.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_data (list[list[str]]): A list of tokenized sentences.\n",
    "\n",
    "    Returns:\n",
    "        set[str]: A set containing the unique words present in the tokenized data.\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for sublist in tokenized_data:\n",
    "        for item in sublist:\n",
    "            vocab.add(item)\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(tokenized_data: list[list[str]], train_ratio: float=0.8,\n",
    "                printing: bool=False) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets based on the specified ratio.\n",
    "    \n",
    "    Parameters:\n",
    "        tokenized_data (list[list[str]]): The input data as a list of lists of tokens.\n",
    "        train_ratio (float): The ratio of data to use for training (between 0 and 1).\n",
    "        printing (bool): If true, information about the split will be printed.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], List[str]]: Train and test sets as lists of sentences or sequences.\n",
    "    \"\"\"\n",
    "    train_size = int(len(tokenized_data) * train_ratio)\n",
    "    train_data = tokenized_data[:train_size]\n",
    "    test_data = tokenized_data[train_size:]\n",
    "\n",
    "    if printing:\n",
    "        print(f\"{len(tokenized_data)} data are split into {len(train_data)} train and {len(test_data)} test set\")\n",
    "\n",
    "        print(\"First training sample:\")\n",
    "        print(train_data[0])\n",
    "            \n",
    "        print(\"First test sample\")\n",
    "        print(test_data[0])\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum Edit Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the minimum edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edit_distance(source: str, target: str, ins_cost: int= 1, \n",
    "                            del_cost: int= 1, rep_cost: int= 2) -> tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Calculate the minimum edit distance between two words.\n",
    "    \n",
    "    Parameters:\n",
    "        source (str): The source word.\n",
    "        target (str): The target word.\n",
    "        ins_cost (int): The cost of insertion (default is 1).\n",
    "        del_cost (int): The cost of deletion (default is 1).\n",
    "        rep_cost (int): The cost of replacement (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    \"\"\"\n",
    "    m, n = len(source), len(target) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    for row in range(1,m+1):\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    for col in range(1,n+1):\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    for row in range(1,m+1): \n",
    "        for col in range(1,n+1):\n",
    "            r_cost = rep_cost\n",
    "            if source[row-1] == target[col-1]:\n",
    "                r_cost = 0\n",
    "            D[row,col] = min(D[row-1,col] + del_cost, D[row,col-1] + ins_cost, D[row-1,col-1] + r_cost)\n",
    "          \n",
    "    med = D[m,n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement one_edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one(word: str, vocabulary: list[str]) -> set[str]:\n",
    "    \"\"\"\n",
    "    Generate a set of possible corrections for a misspelled word with one edit.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The misspelled word.\n",
    "        vocabulary (list[str]): A list of vocabulary words.\n",
    "\n",
    "    Returns:\n",
    "        set[str]: A set of possible corrections.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    deletes = [left + right[1:] for left, right in splits if right]\n",
    "    deletes = [d for d in deletes if is_word(d, vocabulary)]\n",
    "\n",
    "    inserts = [left + c + right for left, right in splits for c in alphabet]\n",
    "    inserts = [i for i in inserts if is_word(i, vocabulary)]\n",
    "\n",
    "    replaces = [left + c + right[1:] for left, right in splits if right for c in alphabet]\n",
    "    replaces = [r for r in replaces if is_word(r, vocabulary)]\n",
    "\n",
    "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right) > 1]\n",
    "    transposes = [t for t in transposes if is_word(t, vocabulary)]\n",
    "    \n",
    "    return set(deletes + inserts + replaces + transposes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate possible corrections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word: str, vocabulary: list[str], n_edits: int=1, \n",
    "                    max_distance: int=2) -> set[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of possible corrections for a misspelled word based on the given vocabulary and maximum edit distance.\n",
    "\n",
    "    Parameters:\n",
    "        word (str): The misspelled word.\n",
    "        vocabulary (list[str]): A list of vocabulary words.\n",
    "        n_edits (int): The number of edits allowed to generate corrections (default is 1).\n",
    "        max_distance (int): The maximum edit distance allowed for a correction to be considered (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        possible_corrections (set[str]): A set of possible corrections.\n",
    "    \"\"\"\n",
    "    possible_corrections = set()\n",
    "\n",
    "    if n_edits == 1:\n",
    "        possible_corrections = {corr for corr in edit_one(word, vocabulary) if calculate_edit_distance(word, corr)[1]<=max_distance}\n",
    "    else:\n",
    "        previous_edits = {word}\n",
    "        for _ in range(n_edits):\n",
    "            current_edits = set()\n",
    "            for prev_edit in previous_edits:\n",
    "                new_corrections = {corr for corr in edit_one(prev_edit, vocabulary) if calculate_edit_distance(word, corr)[1]<=max_distance}\n",
    "                current_edits.update(new_corrections)\n",
    "            previous_edits = current_edits\n",
    "        possible_corrections = previous_edits\n",
    "        \n",
    "    possible_corrections = sorted(possible_corrections, key=lambda corr: calculate_edit_distance(word, corr)[1])\n",
    "\n",
    "    return possible_corrections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences: list[list[str]]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        tokenized_sentences: List of lists of strings.\n",
    "    \n",
    "    Returns:\n",
    "        word_counts: dict that maps word (str) to the frequency (int).\n",
    "    \"\"\"\n",
    "        \n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words with n+ freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences: list[list[str]], \n",
    "                                   freq_threshold: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_sentences: List of lists of sentences.\n",
    "        freq_threshold: Minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        closed_vocab: List of words that appear N times or more.\n",
    "    \"\"\"\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= freq_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oov words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences: list[list[str]], \n",
    "                             vocabulary: list[str], unknown_token: str=\"<unk>\"\n",
    "                             ) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with the unknown token.\n",
    "\n",
    "    Parameters:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        replaced_tokenized_sentences: List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data with unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_with_unk(data: list[list[str]], freq_threshold: int):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by replacing low-frequency words with the unknown token.      \n",
    "    \n",
    "    Parameters:\n",
    "        data: List of lists of strings.\n",
    "        freq_threshold: Words whose count is less than this are treated as unknown.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "   \n",
    "    vocabulary = get_words_with_nplus_frequency(data, freq_threshold)\n",
    "    data_replaced = replace_oov_words_by_unk(data, vocabulary)\n",
    "    \n",
    "    return data_replaced, vocabulary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data: list[list[str]], n: int=2, \n",
    "                  start_token: str='<s>', end_token: str= '<e>'\n",
    "                  ) -> dict:\n",
    "    \"\"\"\n",
    "    Count all n-grams in the given data\n",
    "    \n",
    "    Parameters:\n",
    "        data: List of lists of words.\n",
    "        n: number of words in a sequence (default is 2).\n",
    "        start_token: a string indicate the beginning of the sentence (default is '<s>').\n",
    "        end_token: a string indicate the end of the sentence (default is '<e>').\n",
    "    \n",
    "    Returns:\n",
    "        n_grams: A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = [start_token]*n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        m = len(sentence) if n==1 else len(sentence)-1\n",
    "        for i in range(m): \n",
    "            n_gram = sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word: str, previous_n_gram: list[str], \n",
    "                         n_gram_counts: dict, n_plus1_gram_counts: dict, \n",
    "                         vocabulary_size: int, k: float=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Parameters:\n",
    "        word: Next word\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        vocabulary_size: Number of words in the vocabulary\n",
    "        k: Positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def spell_check_v2(text: str, vocabulary: set[str], top_n: int=2, n_g: int=2,k: int=1.0, \n",
    "                n_edits: int=1, max_distance: int=2) -> tuple[dict, str]:\n",
    "    \"\"\"\n",
    "    Perform spell checking on the given text using an n-gram language model.\n",
    "\n",
    "    Parameters:\n",
    "        text: The text to perform spell checking on.\n",
    "        vocabulary: A set of words representing the vocabulary.\n",
    "        top_n: The number of top suggestions to consider (default is 2).\n",
    "        n_g: The order of the n-gram language model (default is 2).\n",
    "        k: Positive constant, smoothing parameter (default is 1.0).\n",
    "        n_edits: The maximum number of edits allowed in a suggested correction (default is 1).\n",
    "        max_distance: The maximum edit distance allowed for a suggested correction (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        sorted_dict : Dictionary of suggestions.\n",
    "        corrected_text: The corrected version of the input text.\n",
    "    \"\"\"\n",
    "    suggestions = dict()\n",
    "    _, tokenized_sentences = get_tokenized_data(text)\n",
    "    n_grams = count_n_grams(tokenized_sentences, n_g)\n",
    "    n_plus1_grams = count_n_grams(tokenized_sentences, n_g+1)\n",
    "    corrected_text = text\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        index = None\n",
    "        tmp_sentence = ['<s>']*n_g + sentence + ['<e>']\n",
    "        \n",
    "        for token in sentence:\n",
    "            probas = dict()\n",
    "            if not is_word(token, vocabulary):\n",
    "                index = tmp_sentence.index(token)\n",
    "                previous_n_gram = tuple(tmp_sentence[abs(index-n_g):index])\n",
    "                corrections = get_corrections(token, vocabulary, n_edits, max_distance)\n",
    "                corrections = [c for c in corrections if is_word(c, vocabulary)]\n",
    "                for corr in corrections:\n",
    "                    proba = estimate_probability(corr, previous_n_gram, n_grams,\n",
    "                                                 n_plus1_grams, len(vocabulary), k)\n",
    "                    probas[corr] = proba\n",
    "                suggestions[token] = probas\n",
    "        \n",
    "        sorted_suggestions = {k: dict(sorted(v.items(), key=lambda item: item[1], reverse=True)) for k, v in suggestions.items()}\n",
    "        sorted_dict = {}\n",
    "        for key, inner_dict in sorted_suggestions.items():\n",
    "            sorted_inner_dict = dict(sorted(inner_dict.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "            sorted_dict[key] = sorted_inner_dict\n",
    "\n",
    "        for key in sorted_dict.keys():\n",
    "            if key in corrected_text:\n",
    "                first_inner_key = next(iter(sorted_dict[key]))\n",
    "                corrected_text = corrected_text.replace(key, first_inner_key)\n",
    "\n",
    "    return sorted_dict, corrected_text\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('en_US_twitter.txt')\n",
    "sentences, tokenized_data = get_tokenized_data(data)\n",
    "train, test = split_data(tokenized_data, 0.8)\n",
    "vocabulary = get_vocabulary(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"In Krav Maga thre are no rulse, no restrctions.\\nI actaully lked Derek Morris as a Ranger.\"\n",
    "text2 = \"Thanks for the quck birhday lessn\"\n",
    "\n",
    "sorted_dict, corrected_text = spell_check_v2(text1, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "In Krav Maga thre are no rulse, no restrctions.\n",
      "I actaully lked Derek Morris as a Ranger.\n",
      "\n",
      "Corrected text:\n",
      "In Krav Maga threww are no ruse, no restrictions.\n",
      "I actaully led Derek Morris as a Ranger.\n",
      "\n",
      "The misspelled words and thier corrections:\n",
      "--------------------------------------------------\n",
      "  thre:\n",
      "    threw:\t2.3643456673365648e-05\n",
      "    thr:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  rulse:\n",
      "    ruse:\t2.3643456673365648e-05\n",
      "    rule:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  restrctions:\n",
      "    restrictions:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  lked:\n",
      "    led:\t2.3643456673365648e-05\n",
      "    liked:\t2.3643456673365648e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text:\\n{text1}\\n\")\n",
    "print(f\"Corrected text:\\n{corrected_text}\\n\")\n",
    "print(f\"The misspelled words and thier corrections:\")\n",
    "for word in sorted_dict.keys():\n",
    "    print('-'*50)\n",
    "    print(f\"  {word}:\")\n",
    "    for c, p in sorted_dict[word].items():\n",
    "        print(f\"    {c}:\\t{p}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spell checker 3 with LM that models the distribution of letter sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of squences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(sentence: str, lenght:int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    Get the sequences of a sentence.\n",
    "\n",
    "    Parameters:\n",
    "        sentence: a string.\n",
    "        lenght: the length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: a list of sequences.\n",
    "    \"\"\"\n",
    "    #[sentence[i-lenght, i]\n",
    "    sequences = [sentence[i:i+lenght] for i in range(0, len(sentence)-lenght)]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sequences(data: str, lenght: int=2) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the number of word appearence in the tokenized sentences.\n",
    "    \n",
    "    Parameters:\n",
    "        data: dataset, large text(s).\n",
    "        lenght: the lenght of the sequence.\n",
    "    \n",
    "    Returns:\n",
    "        sequence_counts: dict that maps sequence (str) to the frequency (int).\n",
    "    \"\"\"\n",
    "        \n",
    "    sequence_counts = {}\n",
    "    for i in range(lenght, len(data), lenght):\n",
    "        sequence = data[i-lenght, i]\n",
    "        if sequence not in sequence_counts.keys():\n",
    "            sequence_counts[sequence] = 1\n",
    "        else:\n",
    "            sequence_counts[sequence] += 1\n",
    "    \n",
    "    return sequence_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams_v2(sentences: list[str], n: int=2, \n",
    "                  lenght: int = 2, start_token: str='ð', \n",
    "                  end_token: str= '§') -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count all n-grams in the given data\n",
    "    \n",
    "    Parameters:\n",
    "        data: List of words.\n",
    "        n: number of words in a sequence (default is 2).\n",
    "        start_token: a string indicate the beginning of the sentence (default is 'ð').\n",
    "        end_token: a string indicate the end of the sentence (default is '§').\n",
    "    \n",
    "    Returns:\n",
    "        n_grams: A dictionary that maps a tuple of n-words to its frequency\n",
    "    \"\"\"\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = start_token*n + sentence + end_token\n",
    "        sequences = get_sequences(sentence, lenght)\n",
    "        m = len(sequences) if n==1 else len(sequences)-1\n",
    "        for i in range(m): \n",
    "            n_gram = tuple(sequences[i:i+n])\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[tuple(n_gram)] += 1\n",
    "            else:\n",
    "                n_grams[tuple(n_gram)] = 1\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability_v2(sequence: str, previous_n_gram: list[str],\n",
    "                          n_gram_counts: dict, n_plus1_gram_counts: dict, \n",
    "                          num_seq: int, k: float=1.0) -> float:\n",
    "    \"\"\"\n",
    "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
    "    \n",
    "    Parameters:\n",
    "        sequence: Next sequence\n",
    "        previous_n_gram: A sequence of words of length n\n",
    "        n_gram_counts: Dictionary of counts of n-grams\n",
    "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
    "        num_seq: Number of sequences in the dataset\n",
    "        k: Positive constant, smoothing parameter\n",
    "    \n",
    "    Returns:\n",
    "        A probability\n",
    "    \"\"\"\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "    denominator = previous_n_gram_count + k * num_seq\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (sequence,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.gets(n_plus1_gram,0) if n_plus1_gram[0] in n_plus1_gram_counts else 0\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "        \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "def spell_check_v3(text, vocabulary, top_n=2, n_g=2, k=1.0,\n",
    "                length=2, n_edits=1, max_distance=2):\n",
    "    \"\"\"\n",
    "    Perform spell checking on the given text using an n-gram language model.\n",
    "\n",
    "    Parameters:\n",
    "        text: The text to perform spell checking on.\n",
    "        vocabulary: A set of words representing the vocabulary.\n",
    "        top_n: The number of top suggestions to consider (default is 2).\n",
    "        n_g: The order of the n-gram language model (default is 2).\n",
    "        k: Positive constant, smoothing parameter (default is 1.0).\n",
    "        n_edits: The maximum number of edits allowed in a suggested correction (default is 1).\n",
    "        max_distance: The maximum edit distance allowed for a suggested correction (default is 2).\n",
    "\n",
    "    Returns:\n",
    "        sorted_dict: Dictionary of suggestions.\n",
    "        corrected_text: The corrected version of the input text.\n",
    "    \"\"\"\n",
    "    suggestions = {}\n",
    "    sentences, tokenized_sentences = get_tokenized_data(text)\n",
    "    n_grams = count_n_grams_v2(sentences, n=n_g, lenght=length)\n",
    "    n_plus1_grams = count_n_grams_v2(sentences, n=n_g+1, lenght=length)\n",
    "    corrected_text = text\n",
    "    \n",
    "    for sentence, tokenize_sentence in zip(sentences, tokenized_sentences):\n",
    "        index = None\n",
    "        tmp_sentence = 'ð'*n_g + sentence + '§'\n",
    "        tmp_tokenize_sentence = ['ð']*n_g + tokenize_sentence + ['§']\n",
    "        sequences = get_sequences(tmp_sentence, length)\n",
    "        for token in tokenize_sentence:\n",
    "            probas = {}\n",
    "            if not is_word(token, vocabulary):\n",
    "                index = sequences.index(token[0:length])\n",
    "                previous_n_gram = tuple(sequences[max(index-n_g, 0):index])\n",
    "                corrections = get_corrections(token, vocabulary, n_edits, max_distance)\n",
    "                corrections = [c for c in corrections if is_word(c, vocabulary)]\n",
    "                for corr in corrections:\n",
    "                    corr_seq = get_sequences(corr, length)\n",
    "                    proba = estimate_probability_v2(corr_seq, previous_n_gram, n_grams,\n",
    "                                                 n_plus1_grams, len(vocabulary), k)\n",
    "                    probas[corr] = proba\n",
    "                suggestions[token] = probas\n",
    "        \n",
    "        sorted_suggestions = {k: dict(sorted(v.items(), key=lambda item: item[1], reverse=True)) for k, v in suggestions.items()}\n",
    "        sorted_dict = {}\n",
    "        for key, inner_dict in sorted_suggestions.items():\n",
    "            sorted_inner_dict = dict(sorted(inner_dict.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
    "            sorted_dict[key] = sorted_inner_dict\n",
    "\n",
    "        for key in sorted_dict.keys():\n",
    "            if key in corrected_text:\n",
    "                first_inner_key = next(iter(sorted_dict[key]))\n",
    "                corrected_text = re.sub(r'\\b' + re.escape(key) + r'\\b', first_inner_key, corrected_text)\n",
    "\n",
    "    return sorted_dict, corrected_text\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('en_US_twitter.txt', printing=False)\n",
    "sentences, tokenized_data = get_tokenized_data(data)\n",
    "train, test = split_data(tokenized_data, 0.8)\n",
    "vocabulary = get_vocabulary(tokenized_data)\n",
    "length=2\n",
    "n_g = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In Krav Maga thre are no rulse, no restrctions.\\nI actaully lked Derek Morris as a Ranger.\"\n",
    "text2 = \"Thanks for the quck birhday lessn\"\n",
    "\n",
    "sorted_dict, corrected_text = spell_check_v3(text1, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "In Krav Maga thre are no rulse, no restrctions.\n",
      "I actaully lked Derek Morris as a Ranger.\n",
      "\n",
      "Corrected text:\n",
      "In Krav Maga threw are no ruse, no restrictions.\n",
      "I actaully led Derek Morris as a Ranger.\n",
      "\n",
      "The misspelled words and thier corrections:\n",
      "--------------------------------------------------\n",
      "  thre:\n",
      "    threw:\t2.3643456673365648e-05\n",
      "    thr:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  rulse:\n",
      "    ruse:\t2.364289767353887e-05\n",
      "    rule:\t2.364289767353887e-05\n",
      "--------------------------------------------------\n",
      "  restrctions:\n",
      "    restrictions:\t2.3643456673365648e-05\n",
      "--------------------------------------------------\n",
      "  lked:\n",
      "    led:\t2.3643456673365648e-05\n",
      "    liked:\t2.3643456673365648e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text:\\n{text1}\\n\")\n",
    "print(f\"Corrected text:\\n{corrected_text}\\n\")\n",
    "print(f\"The misspelled words and thier corrections:\")\n",
    "for word in sorted_dict.keys():\n",
    "    print('-'*50)\n",
    "    print(f\"  {word}:\")\n",
    "    for c, p in sorted_dict[word].items():\n",
    "        print(f\"    {c}:\\t{p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
